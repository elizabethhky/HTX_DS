{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e7821a-1324-4029-b035-1a2dc6bde7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv-train-2a.ipynb\n",
    "\n",
    "# üß™ 1. Setup\n",
    "# %pip install datasets transformers torchaudio jiwer evaluate --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "773bf08c-4c41-4ac1-a572-3834a8537a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì¶ 2. Imports\n",
    "import os\n",
    "import pandas as pd\n",
    "import torchaudio\n",
    "from datasets import Dataset\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor, TrainingArguments, Trainer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1eefca85-ac59-40d0-91a7-18c0e10ed922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üßπ 3. Load CSV\n",
    "csv_path = \"../asr/common_voice/cv-valid-train.csv\"\n",
    "audio_dir = \"../asr/common_voice/cv-valid-train\"\n",
    "df = pd.read_csv(csv_path)\n",
    "df[\"audio_path\"] = df[\"filename\"].apply(lambda x: os.path.join(audio_dir, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd55e13f-7bab-45d8-a851-4f2a85b768d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>text</th>\n",
       "      <th>up_votes</th>\n",
       "      <th>down_votes</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>accent</th>\n",
       "      <th>duration</th>\n",
       "      <th>audio_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cv-valid-train/sample-000000.mp3</td>\n",
       "      <td>learn to recognize omens and follow them the o...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>../asr/common_voice/cv-valid-train/cv-valid-tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cv-valid-train/sample-000001.mp3</td>\n",
       "      <td>everything in the universe evolved he said</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>../asr/common_voice/cv-valid-train/cv-valid-tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cv-valid-train/sample-000002.mp3</td>\n",
       "      <td>you came so that you could learn about your dr...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>../asr/common_voice/cv-valid-train/cv-valid-tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cv-valid-train/sample-000003.mp3</td>\n",
       "      <td>so now i fear nothing because it was those ome...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>../asr/common_voice/cv-valid-train/cv-valid-tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cv-valid-train/sample-000004.mp3</td>\n",
       "      <td>if you start your emails with greetings let me...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>../asr/common_voice/cv-valid-train/cv-valid-tr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           filename  \\\n",
       "0  cv-valid-train/sample-000000.mp3   \n",
       "1  cv-valid-train/sample-000001.mp3   \n",
       "2  cv-valid-train/sample-000002.mp3   \n",
       "3  cv-valid-train/sample-000003.mp3   \n",
       "4  cv-valid-train/sample-000004.mp3   \n",
       "\n",
       "                                                text  up_votes  down_votes  \\\n",
       "0  learn to recognize omens and follow them the o...         1           0   \n",
       "1         everything in the universe evolved he said         1           0   \n",
       "2  you came so that you could learn about your dr...         1           0   \n",
       "3  so now i fear nothing because it was those ome...         1           0   \n",
       "4  if you start your emails with greetings let me...         3           2   \n",
       "\n",
       "   age gender accent  duration  \\\n",
       "0  NaN    NaN    NaN       NaN   \n",
       "1  NaN    NaN    NaN       NaN   \n",
       "2  NaN    NaN    NaN       NaN   \n",
       "3  NaN    NaN    NaN       NaN   \n",
       "4  NaN    NaN    NaN       NaN   \n",
       "\n",
       "                                          audio_path  \n",
       "0  ../asr/common_voice/cv-valid-train/cv-valid-tr...  \n",
       "1  ../asr/common_voice/cv-valid-train/cv-valid-tr...  \n",
       "2  ../asr/common_voice/cv-valid-train/cv-valid-tr...  \n",
       "3  ../asr/common_voice/cv-valid-train/cv-valid-tr...  \n",
       "4  ../asr/common_voice/cv-valid-train/cv-valid-tr...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5bae021-ca2a-4426-9831-f42fa02f20a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÇÔ∏è 4. Train/Validation Split\n",
    "train_df, val_df = train_test_split(df, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1244c833-6f42-4133-81a4-86e619688b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìö 5. Wrap as HuggingFace Datasets\n",
    "def df_to_dataset(df):\n",
    "    return Dataset.from_dict({\n",
    "        \"path\": df[\"audio_path\"].tolist(),\n",
    "        \"sentence\": df[\"text\"].tolist(),\n",
    "    })\n",
    "\n",
    "train_ds = df_to_dataset(train_df)\n",
    "val_ds = df_to_dataset(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a82a17f9-7655-4891-b969-c266853a15e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('unable to open database file')).History will not be written to the database.\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 28] No space left on device: '../asr/common_voice/cv-valid-train/cv-valid-train/sample-144461.wav'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m             audio \u001b[38;5;241m=\u001b[39m AudioSegment\u001b[38;5;241m.\u001b[39mfrom_mp3(mp3_path)\n\u001b[1;32m     13\u001b[0m             audio\u001b[38;5;241m.\u001b[39mexport(wav_path, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwav\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m convert_all_mp3_to_wav(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../asr/common_voice/cv-valid-train/cv-valid-train\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[15], line 13\u001b[0m, in \u001b[0;36mconvert_all_mp3_to_wav\u001b[0;34m(folder)\u001b[0m\n\u001b[1;32m     11\u001b[0m wav_path \u001b[38;5;241m=\u001b[39m mp3_path\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.mp3\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.wav\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m audio \u001b[38;5;241m=\u001b[39m AudioSegment\u001b[38;5;241m.\u001b[39mfrom_mp3(mp3_path)\n\u001b[0;32m---> 13\u001b[0m audio\u001b[38;5;241m.\u001b[39mexport(wav_path, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwav\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pydub/audio_segment.py:867\u001b[0m, in \u001b[0;36mAudioSegment.export\u001b[0;34m(self, out_f, format, codec, bitrate, parameters, tags, id3v2_version, cover)\u001b[0m\n\u001b[1;32m    861\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m (codec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m parameters \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    862\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m    863\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCan not invoke ffmpeg when export format is \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m; \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    864\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspecify an ffmpeg raw format like format=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ms16le\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m instead \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    865\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mor call export(format=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m) with no codec or parameters\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 867\u001b[0m out_f, _ \u001b[38;5;241m=\u001b[39m _fd_or_path_or_tempfile(out_f, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb+\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    868\u001b[0m out_f\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    870\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pydub/utils.py:60\u001b[0m, in \u001b[0;36m_fd_or_path_or_tempfile\u001b[0;34m(fd, mode, tempfile)\u001b[0m\n\u001b[1;32m     57\u001b[0m     close_fd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fd, basestring):\n\u001b[0;32m---> 60\u001b[0m     fd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(fd, mode\u001b[38;5;241m=\u001b[39mmode)\n\u001b[1;32m     61\u001b[0m     close_fd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 28] No space left on device: '../asr/common_voice/cv-valid-train/cv-valid-train/sample-144461.wav'"
     ]
    }
   ],
   "source": [
    "# üîä 6. Preprocessing Functions\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-large-960h\")\n",
    "\n",
    "from pydub import AudioSegment\n",
    "import os\n",
    "\n",
    "def convert_all_mp3_to_wav(folder):\n",
    "    for file in os.listdir(folder):\n",
    "        if file.endswith(\".mp3\"):\n",
    "            mp3_path = os.path.join(folder, file)\n",
    "            wav_path = mp3_path.replace(\".mp3\", \".wav\")\n",
    "            audio = AudioSegment.from_mp3(mp3_path)\n",
    "            audio.export(wav_path, format=\"wav\")\n",
    "\n",
    "convert_all_mp3_to_wav(\"../asr/common_voice/cv-valid-train/cv-valid-train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af861133-d380-4f07-8c1a-5bb1cbadf7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(batch):\n",
    "    try:\n",
    "        speech_array, sampling_rate = torchaudio.load(batch[\"path\"])\n",
    "        if sampling_rate != 16000:\n",
    "            resampler = torchaudio.transforms.Resample(sampling_rate, 16000)\n",
    "            speech_array = resampler(speech_array)\n",
    "        batch[\"input_values\"] = processor(speech_array.squeeze().numpy(), sampling_rate=16000).input_values[0]\n",
    "        batch[\"labels\"] = processor.tokenizer(batch[\"sentence\"]).input_ids\n",
    "        return batch\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping file {batch['path']}: {e}\")\n",
    "        return {\"input_values\": None, \"labels\": None}\n",
    "\n",
    "train_ds = train_ds.map(preprocess, num_proc=4, load_from_cache_file=False)\n",
    "val_ds = val_ds.map(preprocess, num_proc=4, load_from_cache_file=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f2d6d7-a042-40d5-8da8-82a894f426cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß† 7. Load Model\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-large-960h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517ad9fa-9cb6-4596-ba78-37c8bd6ee31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚öôÔ∏è 8. Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./wav2vec2-large-960h-cv\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-4,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88610d17-5333-43dd-a314-ed022e897a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìè 9. Metric\n",
    "wer = evaluate.load(\"wer\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_ids = np.argmax(pred.predictions, axis=-1)\n",
    "    pred_str = processor.batch_decode(pred_ids)\n",
    "    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
    "    return {\"wer\": wer.compute(predictions=pred_str, references=label_str)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3632076-82c4-4ee5-8178-1454a8d4c28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üèãÔ∏è 10. Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=processor.feature_extractor,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5186905a-d659-422d-98d6-d68247f54415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üíæ 11. Save Fine-Tuned Model\n",
    "model.save_pretrained(\"./wav2vec2-large-960h-cv\")\n",
    "processor.save_pretrained(\"./wav2vec2-large-960h-cv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27751dac-9e35-4806-a97a-95abf5ea7b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîç 12. Load and Transcribe Test Set\n",
    "test_csv = \"../common_voice/cv-valid-test/cv-valid-test.csv\"\n",
    "test_dir = \"../common_voice/cv-valid-test\"\n",
    "test_df = pd.read_csv(test_csv)\n",
    "test_df[\"audio_path\"] = test_df[\"path\"].apply(lambda x: os.path.join(test_dir, x))\n",
    "\n",
    "def transcribe(file_path):\n",
    "    speech_array, sampling_rate = torchaudio.load(file_path)\n",
    "    inputs = processor(speech_array.squeeze().numpy(), sampling_rate=16000, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "    pred_ids = torch.argmax(logits, dim=-1)\n",
    "    transcription = processor.batch_decode(pred_ids)[0]\n",
    "    return transcription\n",
    "\n",
    "test_df[\"generated_text\"] = test_df[\"audio_path\"].apply(transcribe)\n",
    "\n",
    "test_df.to_csv(\"cv-valid-test-with-predictions.csv\", index=False)\n",
    "print(\"‚úÖ Test predictions saved to cv-valid-test-with-predictions.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
